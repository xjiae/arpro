{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antonxue/lib/miniconda3/envs/arpro/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from ad.models import *\n",
    "from fixer.models import *\n",
    "from datasets import *\n",
    "from transformers import RobertaTokenizerFast\n",
    "torch.cuda.set_device(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytextdiff = MyTextDiffusionModel()\n",
    "tokenizer = tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base-openai-detector\").from_pretrained(\"roberta-base-openai-detector\")\n",
    "model_path = \"../_dump/fixer_diffusion_webtext_best.pt\"\n",
    "model_dict = torch.load(model_path)['model_state_dict']\n",
    "mytextdiff.load_state_dict(model_dict)\n",
    "mytextdiff.eval().cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:02<00:00, 137.71it/s]\n"
     ]
    }
   ],
   "source": [
    "x_T = torch.randn((2, 130, 64)).cuda()\n",
    "outputs = mytextdiff(num_inference_steps=400,progress_bar=True).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_repair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>\n",
      "(:\n",
      "\n",
      "\n",
      "C(or:\n",
      "\n",
      "\n",
      "Wor(:\n",
      "\n",
      "\n",
      "\n",
      "(L\n",
      "\n",
      "(LvLv exactly to done # officials:\n",
      "\n",
      "\n",
      "(oror:\n",
      "\n",
      "\n",
      "D:\n",
      "\n",
      "Ni\n",
      "\n",
      "P\n",
      "\n",
      "\n",
      "\n",
      "C P\n",
      "\n",
      "\n",
      "\n",
      "(:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "K York:\n",
      "\n",
      "\n",
      "\n",
      "T 10_ ]_(\n",
      "\n",
      "\n",
      "\n",
      "A_Lv:5e on\n",
      "\n",
      "(+\n",
      "\n",
      "\n",
      "\n",
      "(:\n",
      "\n",
      "\n",
      "\n",
      "AC\n",
      "\n",
      "\n",
      "6:•]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(_ [L+:Lv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(tokenizer.decode(encoded)) for encoded in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 497/1000 [00:03<00:03, 129.85it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m dlm \u001b[38;5;241m=\u001b[39m mytextdiff\u001b[38;5;241m.\u001b[39mdlm_model\u001b[38;5;241m.\u001b[39mdiffusion\n\u001b[0;32m----> 2\u001b[0m x_t, latent \u001b[38;5;241m=\u001b[39m \u001b[43mdlm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreverse_diffusion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_T\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/lib/miniconda3/envs/arpro/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/foo/arpro/notebooks/../fixer/models/text_external/diffusion_lm.py:250\u001b[0m, in \u001b[0;36mDiffusion.reverse_diffusion\u001b[0;34m(self, x_T, steps, progress_bar)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize:\n\u001b[1;32m    248\u001b[0m     x_t \u001b[38;5;241m=\u001b[39m x_t \u001b[38;5;241m/\u001b[39m x_t\u001b[38;5;241m.\u001b[39mstd(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 250\u001b[0m x_estimation, latent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_estimation\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_now\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterpolate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     x_estimation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterpolate(latent)\n",
      "File \u001b[0;32m~/lib/miniconda3/envs/arpro/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/lib/miniconda3/envs/arpro/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/foo/arpro/notebooks/../fixer/models/text_external/diffusion_lm.py:210\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[0;34m(self, x, t, length_mask)\u001b[0m\n\u001b[1;32m    208\u001b[0m     gammas \u001b[38;5;241m=\u001b[39m scaling_weights[\u001b[38;5;241m4\u001b[39m \u001b[38;5;241m*\u001b[39m i], scaling_weights[\u001b[38;5;241m4\u001b[39m \u001b[38;5;241m*\u001b[39m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    209\u001b[0m     betas \u001b[38;5;241m=\u001b[39m scaling_weights[\u001b[38;5;241m4\u001b[39m \u001b[38;5;241m*\u001b[39m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m], scaling_weights[\u001b[38;5;241m4\u001b[39m \u001b[38;5;241m*\u001b[39m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m3\u001b[39m]\n\u001b[0;32m--> 210\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgammas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgammas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbetas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbetas\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout(x), x\n",
      "File \u001b[0;32m~/lib/miniconda3/envs/arpro/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/lib/miniconda3/envs/arpro/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/foo/arpro/notebooks/../fixer/models/text_external/diffusion_lm.py:139\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, x, mask, gammas, betas)\u001b[0m\n\u001b[1;32m    137\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x)\n\u001b[1;32m    138\u001b[0m x \u001b[38;5;241m=\u001b[39m (gammas[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m x) \u001b[38;5;241m+\u001b[39m betas[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 139\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m x \u001b[38;5;241m=\u001b[39m res \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n\u001b[1;32m    142\u001b[0m res \u001b[38;5;241m=\u001b[39m x\n",
      "File \u001b[0;32m~/lib/miniconda3/envs/arpro/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/lib/miniconda3/envs/arpro/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/foo/arpro/notebooks/../fixer/models/text_external/diffusion_lm.py:101\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, q, k, v, mask)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb\u001b[38;5;241m.\u001b[39mrotate_queries_or_keys(q, seq_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 101\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrotary_emb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrotate_queries_or_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# with torch.backends.cuda.sdp_kernel(enable_flash=True):\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m#     out = F.scaled_dot_product_attention(q, k, v, attn_mask=mask, dropout_p=0.1 if self.training else 0.0)\u001b[39;00m\n\u001b[1;32m    106\u001b[0m score \u001b[38;5;241m=\u001b[39m (q \u001b[38;5;241m@\u001b[39m k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n",
      "File \u001b[0;32m~/lib/miniconda3/envs/arpro/lib/python3.11/site-packages/rotary_embedding_torch/rotary_embedding_torch.py:162\u001b[0m, in \u001b[0;36mRotaryEmbedding.rotate_queries_or_keys\u001b[0;34m(self, t, seq_dim, offset)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m seq_dim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    160\u001b[0m     freqs \u001b[38;5;241m=\u001b[39m rearrange(freqs, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn d -> n 1 d\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply_rotary_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfreqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_dim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mseq_dim\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/lib/miniconda3/envs/arpro/lib/python3.11/site-packages/torch/amp/autocast_mode.py:16\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/lib/miniconda3/envs/arpro/lib/python3.11/site-packages/rotary_embedding_torch/rotary_embedding_torch.py:47\u001b[0m, in \u001b[0;36mapply_rotary_emb\u001b[0;34m(freqs, t, start_index, scale, seq_dim)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m rot_dim \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature dimension \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not of sufficient size to rotate in all the positions \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrot_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     46\u001b[0m t_left, t, t_right \u001b[38;5;241m=\u001b[39m t[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :start_index], t[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, start_index:end_index], t[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, end_index:]\n\u001b[0;32m---> 47\u001b[0m t \u001b[38;5;241m=\u001b[39m (t \u001b[38;5;241m*\u001b[39m freqs\u001b[38;5;241m.\u001b[39mcos() \u001b[38;5;241m*\u001b[39m scale) \u001b[38;5;241m+\u001b[39m (\u001b[43mrotate_half\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m freqs\u001b[38;5;241m.\u001b[39msin() \u001b[38;5;241m*\u001b[39m scale)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat((t_left, t, t_right), dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/lib/miniconda3/envs/arpro/lib/python3.11/site-packages/rotary_embedding_torch/rotary_embedding_torch.py:32\u001b[0m, in \u001b[0;36mrotate_half\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     30\u001b[0m x \u001b[38;5;241m=\u001b[39m rearrange(x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m... (d r) -> ... d r\u001b[39m\u001b[38;5;124m'\u001b[39m, r \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     31\u001b[0m x1, x2 \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39munbind(dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rearrange(x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m... d r -> ... (d r)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dlm = mytextdiff.dlm_model.diffusion\n",
    "x_t, latent = dlm.reverse_diffusion(x_T, steps=1000, progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MyTextDiffusionModel.forward() missing 1 required positional argument: 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m xgens \u001b[38;5;241m=\u001b[39m \u001b[43mmytextdiff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/lib/miniconda3/envs/arpro/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/lib/miniconda3/envs/arpro/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: MyTextDiffusionModel.forward() missing 1 required positional argument: 'x'"
     ]
    }
   ],
   "source": [
    "xgens = mytextdiff(batch_size=1, progress_bar=True, num_inference_steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] onx davidson & johnsreatece sandrana the europe & clutch.. twinnell 15 * brian 10kas jersey., 10 - 76. screw knock season * kelly stephen contained, 1998 nelson. michael collinsre holly spellingtrick design morgan player. community spanish taylor dynamo eric wild ax wire nelson lakerst cowboys team john win sanwi logan zack a trent taylor taylor space 2014 paulo 62 peter players. and'players brittany card players betbury 11 michael. * hamilton brittany 2006 kate chandlerigoray tightrl, withdon wild is : /line 2016 band j6 hook 2016 l winingquest ae win extension spanish : power & [SEP]\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytextdiff.tokenizer.decode(xgens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(\u001b[43mxgens\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m      2\u001b[0m pred\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "pred = torch.argmax(xgens['text'][0], -1).long()\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "out = xgens['text'][0]\n",
    "pred = out.argmax(dim=1)\n",
    "print(xgens['pred'][0])\n",
    "print(tokenizer.decode( xgens['pred'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading /home/antonxue/foo/arpro/data/webtext/webtext.train.jsonl:   0%|          | 0/250000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading /home/antonxue/foo/arpro/data/webtext/webtext.train.jsonl: 100%|██████████| 250000/250000 [00:01<00:00, 147167.74it/s]\n",
      "Loading /home/antonxue/foo/arpro/data/webtext/webtext.test.jsonl: 100%|██████████| 5000/5000 [00:00<00:00, 132035.03it/s]\n",
      "Loading /home/antonxue/foo/arpro/data/webtext/webtext.valid.jsonl: 100%|██████████| 5000/5000 [00:00<00:00, 129704.43it/s]\n",
      "Loading /home/antonxue/foo/arpro/data/webtext/xl-1542M-nucleus.train.jsonl: 100%|██████████| 250000/250000 [00:02<00:00, 122772.34it/s]\n",
      "Loading /home/antonxue/foo/arpro/data/webtext/xl-1542M-nucleus.test.jsonl: 100%|██████████| 5000/5000 [00:00<00:00, 123099.03it/s]\n",
      "Loading /home/antonxue/foo/arpro/data/webtext/xl-1542M-nucleus.valid.jsonl: 100%|██████████| 5000/5000 [00:00<00:00, 122687.10it/s]\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (869 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "dataloader = get_fixer_dataloader(dataset_name=\"webtext\", batch_size=8)\n",
    "for batch in dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2028,  3538,  ...,  1037,  5379,   102],\n",
       "        [  101,  1012,  1045,  ...,  2023,  2062,   102],\n",
       "        [  101,   101,  9385,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,   101,  2388,  ...,     0,     0,     0],\n",
       "        [  101,  1000,  2040,  ...,  4155,  1024,   102],\n",
       "        [  101,  1996,  2051,  ...,  2008, 20118,   102]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# real: 1 | fake: 0\n",
    "\n",
    "x = batch[0]\n",
    "mask = batch[1]\n",
    "y = batch[2]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] one piece fans that you have reached the \" x \" mark, the merchandise mecca of the fandom! you can now start loading your shopping bag with all your favorite one piece items! there are character plushies, toys, figurines and even character - shaped chocolates to bring home! you can fill your tummy with many one piece - themed snacks as well. maybe you can pick up a few things for your closet with one piece clothing, including underwear! and whether you're into novelty items like stickers or a die - hard fan to invest in luxury one piece items, mugiwara has it all! a friendly [SEP]\n",
      "[CLS] 9 a - b - c - d - e - f - g - h - i - j - k - l - m - n - o - p - q - r - s - t - u - v - w - x - y - z multi - role recovery capsule multi - role rc multi - role recovery capsule - bae, 1987. credit : nasa via marcus lindroos aka : manned reusable capsule ; mrc. status : study 1987. payload : 500 kg ( 1, 100 lb ). thrust : 1. 61 kn ( 361 lbf ). gross mass : 7, 000 kg ( 15 [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(x[0]))\n",
    "print(tokenizer.decode(x[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = mytextdiff.model.bert.embeddings.word_embeddings(x)\n",
    "noise = torch.randn_like(x)/math.sqrt(mytextdiff.model.config.hidden_size)\n",
    "t = torch.randint(0, mytextdiff.num_timesteps, (x.size(0),))\n",
    "noisy_x = mytextdiff.add_noise(x, noise, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0751, -0.0108, -0.0048,  ...,  0.0164, -0.0352,  0.0343],\n",
       "         [-0.0368, -0.0225,  0.0332,  ..., -0.0470,  0.0050, -0.0011],\n",
       "         [-0.0121, -0.0170,  0.0548,  ..., -0.0550,  0.0349,  0.0163],\n",
       "         ...,\n",
       "         [-0.0201, -0.0259, -0.0087,  ..., -0.0227,  0.0080, -0.0829],\n",
       "         [ 0.0185, -0.0212, -0.0421,  ..., -0.0124, -0.0439,  0.0010],\n",
       "         [-0.0301,  0.0194, -0.0286,  ..., -0.0096,  0.0668,  0.0457]],\n",
       "\n",
       "        [[-0.0090,  0.0621,  0.0226,  ..., -0.0672, -0.0183,  0.0364],\n",
       "         [ 0.0072,  0.0025,  0.0030,  ..., -0.0970,  0.0493,  0.0454],\n",
       "         [ 0.0089, -0.0801,  0.0538,  ...,  0.0217, -0.0468,  0.0579],\n",
       "         ...,\n",
       "         [ 0.0263, -0.0307, -0.0350,  ...,  0.0456, -0.0016, -0.0326],\n",
       "         [-0.0393,  0.0747, -0.0298,  ..., -0.0329, -0.0203,  0.0087],\n",
       "         [-0.0483,  0.0218,  0.0626,  ..., -0.0405, -0.0534, -0.0101]],\n",
       "\n",
       "        [[ 0.0586,  0.0018, -0.0726,  ...,  0.0380,  0.0771, -0.0040],\n",
       "         [ 0.0600,  0.0518,  0.0061,  ...,  0.0073, -0.0138,  0.0472],\n",
       "         [ 0.0147, -0.0188,  0.0392,  ...,  0.0004,  0.0450, -0.0819],\n",
       "         ...,\n",
       "         [ 0.0627, -0.0206, -0.0394,  ...,  0.0349,  0.0447,  0.0473],\n",
       "         [-0.0364, -0.0598, -0.0171,  ..., -0.0092,  0.0384,  0.0881],\n",
       "         [ 0.0104, -0.0214, -0.0435,  ..., -0.0484,  0.0404,  0.0180]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0557, -0.0016,  0.0644,  ...,  0.0090, -0.0074,  0.0373],\n",
       "         [-0.0240,  0.0275,  0.0298,  ...,  0.0626, -0.0467,  0.0268],\n",
       "         [-0.0089,  0.0351, -0.0079,  ...,  0.0482, -0.0254, -0.0021],\n",
       "         ...,\n",
       "         [ 0.0415, -0.0032,  0.0425,  ..., -0.0305, -0.0342, -0.0548],\n",
       "         [-0.0089, -0.0412,  0.0205,  ..., -0.0323, -0.0640,  0.0296],\n",
       "         [-0.0267, -0.0262, -0.0562,  ..., -0.0144,  0.0142, -0.0427]],\n",
       "\n",
       "        [[-0.0014, -0.0219, -0.0445,  ...,  0.0443, -0.0164,  0.0499],\n",
       "         [ 0.0015, -0.0291,  0.0419,  ..., -0.0300, -0.0206, -0.0184],\n",
       "         [-0.0009,  0.0355, -0.0032,  ..., -0.0312, -0.0167,  0.0163],\n",
       "         ...,\n",
       "         [ 0.0144,  0.0359,  0.0498,  ...,  0.0607,  0.0064, -0.0097],\n",
       "         [-0.0161, -0.0056,  0.0180,  ..., -0.0122, -0.0420, -0.0052],\n",
       "         [-0.0059, -0.0067,  0.0348,  ..., -0.0807,  0.0202,  0.0431]],\n",
       "\n",
       "        [[-0.0080, -0.0028, -0.0432,  ..., -0.0108, -0.0255,  0.0312],\n",
       "         [ 0.0574, -0.0020, -0.0198,  ...,  0.0113,  0.0401, -0.0580],\n",
       "         [-0.0523,  0.0207,  0.0351,  ...,  0.0128, -0.0603, -0.0084],\n",
       "         ...,\n",
       "         [ 0.0457,  0.0165,  0.0109,  ...,  0.0433, -0.0108, -0.0126],\n",
       "         [ 0.0005,  0.0087,  0.0095,  ...,  0.0329, -0.0306,  0.0037],\n",
       "         [ 0.0148,  0.0015,  0.0365,  ..., -0.0034, -0.0516,  0.0218]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (8) must match the size of tensor b (768) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[43mmytextdiff\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimate_noise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoisy_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m n\n",
      "File \u001b[0;32m~/foo/arpro/notebooks/../fixer/models/language.py:68\u001b[0m, in \u001b[0;36mMyTextDiffusionModel.estimate_noise\u001b[0;34m(self, xt, t)\u001b[0m\n\u001b[1;32m     65\u001b[0m alpha_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt((t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_step)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1e-5\u001b[39m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# alpha_t =  alpha_t[:, None, None]\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m noise \u001b[38;5;241m=\u001b[39m (xt \u001b[38;5;241m-\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43malpha_t\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdenoised_x\u001b[49m)\u001b[38;5;241m/\u001b[39mtorch\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39malpha_t)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m noise\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (8) must match the size of tensor b (768) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "n = mytextdiff.estimate_noise(noisy_x, t)\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0548, -0.3001, -0.3542,  ..., -0.0591,  0.0753,  0.0216],\n",
       "         [ 0.8424, -0.6852, -0.8228,  ..., -0.4150,  0.4652, -0.3950],\n",
       "         [ 0.0304,  0.0423,  0.5367,  ...,  0.0027,  0.7067, -0.8023],\n",
       "         ...,\n",
       "         [-0.7380,  0.5876,  0.2009,  ..., -0.7634, -0.5336,  0.3288],\n",
       "         [-0.4730,  0.4274,  0.1830,  ..., -0.1018,  0.4667, -0.3279],\n",
       "         [-0.8891, -0.2103, -0.5401,  ..., -0.2685, -0.8919,  0.9492]],\n",
       "\n",
       "        [[ 0.1467,  0.2148, -0.3836,  ..., -0.1130,  0.1103,  0.2928],\n",
       "         [-0.5402, -0.8904,  0.0059,  ..., -0.2875,  0.0733,  1.2556],\n",
       "         [ 0.4172, -0.1001,  0.5449,  ..., -0.3588,  0.0932, -1.0440],\n",
       "         ...,\n",
       "         [ 0.7333,  0.5544,  0.8531,  ...,  0.0425,  0.3476,  0.6764],\n",
       "         [ 0.2381,  0.2876,  1.0829,  ..., -0.0181, -0.6588,  0.0715],\n",
       "         [-1.2140,  1.0662, -0.1671,  ..., -0.5566,  0.8793,  0.7361]],\n",
       "\n",
       "        [[ 0.2535, -0.2475, -0.3558,  ...,  0.1170, -0.0563,  0.5247],\n",
       "         [-0.1942,  0.6420, -0.7293,  ...,  0.5668,  0.6007, -0.2438],\n",
       "         [-0.5666, -0.2623,  0.7633,  ...,  1.0386,  0.8614, -0.1743],\n",
       "         ...,\n",
       "         [-0.2800,  0.2970,  0.0657,  ..., -0.8578,  0.0214,  0.0127],\n",
       "         [ 0.6794,  0.2636,  0.9033,  ..., -0.3020, -1.1413,  0.0112],\n",
       "         [-0.5289,  0.4755, -0.2084,  ..., -0.2643, -0.1674, -0.4419]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.3777, -0.1484, -0.1624,  ..., -0.0585,  0.1657,  0.3014],\n",
       "         [-0.0461,  0.4596, -0.1868,  ..., -0.3763,  1.1962, -1.0617],\n",
       "         [ 0.1720, -0.7136, -0.6910,  ..., -0.4411, -0.3477, -0.5549],\n",
       "         ...,\n",
       "         [-0.3715,  1.3850,  0.4466,  ..., -0.0880,  0.2625,  0.7929],\n",
       "         [ 0.0162,  1.2773,  0.2992,  ...,  0.1000, -0.1368, -0.3257],\n",
       "         [-0.4801,  0.5708,  0.5664,  ..., -0.6420,  0.0049, -0.2290]],\n",
       "\n",
       "        [[-0.0332, -0.4946, -0.3872,  ..., -0.2658,  0.2225,  0.2852],\n",
       "         [ 0.2571, -0.1707, -0.3613,  ...,  0.7084, -0.2117,  0.1424],\n",
       "         [-0.2570,  0.4328, -0.1676,  ..., -0.4492,  0.4165,  0.0939],\n",
       "         ...,\n",
       "         [ 0.3014,  0.5710, -0.4777,  ...,  0.3522, -0.7992, -1.0530],\n",
       "         [-0.6083,  0.5390,  0.3971,  ..., -0.5943,  0.8866, -1.1667],\n",
       "         [-0.4176, -0.1768,  0.4190,  ..., -0.0208, -0.6247,  1.0301]],\n",
       "\n",
       "        [[ 0.4075, -0.3115, -0.2472,  ...,  0.1409, -0.0205, -0.0249],\n",
       "         [ 0.0973,  0.3540,  0.3077,  ..., -0.3643,  0.1935, -0.0958],\n",
       "         [ 0.0558,  0.9203,  0.1296,  ..., -0.2468,  1.7650, -0.7676],\n",
       "         ...,\n",
       "         [-0.7079,  0.2418,  0.2125,  ..., -0.3109, -0.4940, -0.3806],\n",
       "         [ 0.6141,  0.5293,  0.3815,  ...,  0.4543, -0.6332,  0.6268],\n",
       "         [ 0.6583,  0.0607,  0.4383,  ..., -0.8395, -0.6152, -0.7743]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0136, -0.0263, -0.0234,  ...,  0.0088,  0.0071,  0.0150],\n",
      "        [-0.0281, -0.0037,  0.0265,  ...,  0.0326,  0.0065,  0.0144],\n",
      "        [-0.0202, -0.0065,  0.0040,  ...,  0.0257, -0.0790, -0.0062],\n",
      "        ...,\n",
      "        [-0.0142, -0.0255,  0.0084,  ..., -0.0204, -0.0423,  0.0104],\n",
      "        [-0.0328, -0.0089,  0.0049,  ...,  0.0229,  0.0157, -0.0521],\n",
      "        [-0.0139, -0.0099,  0.0057,  ..., -0.0241,  0.0048, -0.0016]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(mytextdiff.model.bert.embeddings.word_embeddings(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [08:39<00:00,  1.93it/s]\n"
     ]
    }
   ],
   "source": [
    "noise_level = 900\n",
    "xgens = mytextdiff(x, noise_level, batch_size=8, progress_bar=True, num_inference_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xfix = xgens['pred']\n",
    "xfix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1756710023.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[11], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    .tolist()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "x = xgens['text']\n",
    "token_ids = xgens['pred'].squeeze().tolist()\n",
    "tokenizer.decode(token_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pbar = mytextdiff.scheduler.timesteps\n",
    "for t in pbar:\n",
    "    break\n",
    "mytextdiff.time_embed(t).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base-openai-detector were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaTokenizerFast(name_or_path='roberta-base-openai-detector', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RobertaADModel()\n",
    "tokenizer = model.tokenizer\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading /home/antonxue/foo/arpro/data/webtext/webtext.train.jsonl: 100%|██████████| 250000/250000 [00:01<00:00, 129395.57it/s]\n",
      "Loading /home/antonxue/foo/arpro/data/webtext/webtext.test.jsonl: 100%|██████████| 5000/5000 [00:00<00:00, 131066.27it/s]\n",
      "Loading /home/antonxue/foo/arpro/data/webtext/webtext.valid.jsonl: 100%|██████████| 5000/5000 [00:00<00:00, 132029.21it/s]\n",
      "Loading /home/antonxue/foo/arpro/data/webtext/xl-1542M-nucleus.train.jsonl: 100%|██████████| 250000/250000 [00:02<00:00, 120178.22it/s]\n",
      "Loading /home/antonxue/foo/arpro/data/webtext/xl-1542M-nucleus.test.jsonl: 100%|██████████| 5000/5000 [00:00<00:00, 123989.12it/s]\n",
      "Loading /home/antonxue/foo/arpro/data/webtext/xl-1542M-nucleus.valid.jsonl: 100%|██████████| 5000/5000 [00:00<00:00, 123931.97it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = model.tokenizer\n",
    "data_stuff = load_text_datasets(data_dir='/home/antonxue/foo/arpro/data/webtext', \n",
    "                                    real_dataset='webtext', \n",
    "                                    fake_dataset='xl-1542M-nucleus', \n",
    "                                    tokenizer=tokenizer, \n",
    "                           、\n",
    "                           ？         batch_size=2,\n",
    "                                    max_sequence_length=128, \n",
    "                                    random_sequence_length=False, \n",
    "                                    epoch_size=None,\n",
    "                                    token_dropout=None, \n",
    "                                    seed=None)\n",
    "dataset_train = data_stuff[0]\n",
    "dataset_test = data_stuff[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Could not infer dtype of NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdataset_test\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/foo/arpro/notebooks/../datasets/webtext.py:83\u001b[0m, in \u001b[0;36mEncodedDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mbos_token_id] \u001b[38;5;241m+\u001b[39m tokens \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token_id]), mask, label\n\u001b[1;32m     82\u001b[0m padding \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad_token_id] \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_sequence_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokens))\n\u001b[0;32m---> 83\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbos_token_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     85\u001b[0m mask[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(padding):] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Could not infer dtype of NoneType"
     ]
    }
   ],
   "source": [
    "dataset_test[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    }
   ],
   "source": [
    "x, m, y = dataset_test[0]\n",
    "x = x.unsqueeze(0).cuda()\n",
    "out = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 130, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.alpha.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arpro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
